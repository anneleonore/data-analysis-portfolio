{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee3a91e-a61e-4bee-892e-39c48504721c",
   "metadata": {},
   "source": [
    "# Data Analysis Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd32e9-b78b-4a71-9663-b958235f5b66",
   "metadata": {},
   "source": [
    "This notebook is part of my data analysis portfolio, where I explore **three** key areas:\n",
    "1. Data Processing and Visualization\n",
    "2. Traditional Machine Learning and Deep Learning\n",
    "3. Text Sentiment and Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b13ebcc-e748-4be4-bcad-913d39335fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import general packages\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694eb9a-2dfa-40a6-901e-57f57eecea94",
   "metadata": {},
   "source": [
    "## <span style=\"background-color: #FFE5B4 \">  Section 1. Data Processing and Visualization </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7aeb4-64be-459a-be5f-51dd5dc26f68",
   "metadata": {},
   "source": [
    "### General information\n",
    "There are various important packages for *data processing and visualization*. In the example code below, I will be focusing on:\n",
    "- pandas\n",
    "- numpy\n",
    "- statsmodel\n",
    "- matplotlib\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f29eca-8a95-4478-8d71-5f8f6115259a",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4b51b-bb6a-43eb-923d-5f02c643fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages/modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941599b4-e905-4305-bce6-8f3b43e29802",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">1.1 Data Processing</span>\n",
    "\n",
    "<a href=\"https://www.fullstory.com/blog/what-is-data-processing/\">Data processing</a> is a series of operations performed on data to transform, analyze, and organze it in a useful format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57e068-8120-4820-baa3-25b4d2f0d5b2",
   "metadata": {},
   "source": [
    "#### 1.1.1 Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a208a8-7ea8-448e-8c47-91a00ec3f9fa",
   "metadata": {},
   "source": [
    "#### 1.1.2 Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe56dec-caab-4416-bb34-3c67ebd57d47",
   "metadata": {},
   "source": [
    "#### 1.1.3 Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde09302-4c65-4d28-b951-0737a778b123",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">1.2 Data Visualization</span>\n",
    "\n",
    "<a href=\"https://www.tableau.com/learn/articles/data-visualization\">Data visualization</a> is the graphical representation of data through use of visual elements, such as charts, graphs, plots, and infographics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a784b-55f9-442c-a077-ece6db194c1c",
   "metadata": {},
   "source": [
    "#### 1.2.1 Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb5e65-52f6-4fdf-8e10-2730870a12c5",
   "metadata": {},
   "source": [
    "#### 1.2.2 Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8aa97-2568-4af0-b5d9-f19dca88aea9",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0.8px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc6c66-b979-4a0f-9746-deb18aaacab1",
   "metadata": {},
   "source": [
    "## <span style=\"background-color: #FFE5B4 \"> Section 2. Traditional Machine Learning and Deep Learning </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34669a01-d0ee-44b6-a16d-958a97da61fe",
   "metadata": {},
   "source": [
    "### General information\n",
    "There are various important packages for *traditional machine learning and deep learning*. In the example code below, I will be focusing on:\n",
    "- pytorch\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2139b-0343-41ee-884d-615ec4461010",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c29ffa8-1e6c-4c90-b0fd-2527aeada754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import packages/modules\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Import specific objects\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de3563-ca31-4af8-aad0-ef66763b6648",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">2.1 Traditional Machine Learning</span>\n",
    "scikit-learn is focused on traditional machine learning tasks, such as linear regression, clustering, and support vector machines (CVMs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1efb62-8246-44f5-a825-67de2892d6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba05eaf-1b24-438d-bb8f-3da5c2545ec2",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">2.2 Deep Learning</span>\n",
    "PyTorch is primarily designed for deep learning tasks, such as neural networks (CNNs, RNNs) and transformers (BERT, RoBERTa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ce277-5b3b-44bb-b743-f04d5c8ec9e5",
   "metadata": {},
   "source": [
    "#### Important terminology: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab285f58-46ef-4424-b3ad-6b814e007ced",
   "metadata": {},
   "source": [
    "- **autograd**: Computes the gradients (slopes) of the loss function with respect to the model's weights.\n",
    "  - Important: When a forward function is defined in PyTorch, the backward function is automatically generated by PyTorch's autograd system, so the backward function doesn't need to be explicitly defined.\n",
    "- **backpropagation**: The process of adjusting the weights of a neural network by analyzing the error rate from the previous iteration.\n",
    "- **batch**: A hyperparameter that defines the number of samples that are processed before the interal model parameters are updated.\n",
    "- **Dataset**: Data primitive that stores the samples and their corresponding labels.\n",
    "- **DataLoader**: Data primitive that wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "- **gradient descent**: An iterative optimization method that minimises the loss function in machine learning models.\n",
    "- **epoch**: A hyperparameter that defines the number of complete passes through the training dataset.\n",
    "- **hyperparameter**: A parameter that is set before the machine learning process begins.\n",
    "- **learning rate**: A hyperparameter that controls the step size of each gradient descent update.\n",
    "- **loss function**: A mathematical function that measures the difference between the model's predictions and the actual labels. In other words, it computes a value that estimates how far the output is from the target.\n",
    "  - Common loss functions: nn.CrossEntropyLoss() | nn.MSELoss() | nn.BCELoss() | nn.L1Loss()\n",
    "- **Model**: A neural network architecture that is designed to solve a specific problem.\n",
    "- **module**: Base class for all neural network models (the building blocks).\n",
    "- **neural network (NN)**: A machine learning program/model that makes decisions in a manner similar to the human brain.\n",
    "- **optimizer**: A tool that helps with the process of training a machine learning model.\n",
    "  - SGD (stochastic gradient descent) is an optimizer that updates model weights based on the gradient of the loss function.\n",
    "  - torch.optim provides a wide range of optimizers.\n",
    "- **parameter**: \n",
    "- **propagation**\n",
    "    - **forward propagation**: NN makes best guess about the correct output. It runs the input through each of the functions to make this guess.\n",
    "    - **backward propagation**: NN adjusts its parameters proportionate to the error in its guess, making bigger/smaller changes for bigger/smaller errors. Backpropagation relies on autograd to compute gradients, which are then used to update the model's weights.\n",
    "    - loss.backward() implicitly generates the backward function.\n",
    "- **sample**: A single row of data.\n",
    "- **tensor**: A multi-dimensional array of numerical values (a \"container\" for data) that run on GPU to accelerate computing.\n",
    "    - A tensor can be created by running: torch.tensor(data) | torch.ones(r,c) | torch.zero(r,c) | torch.rand(r,c).\n",
    "    - A tensor can also be created from a numpy array by running: torch.from_numpy(np_array)\n",
    "    - Tensors of similar shapes can be added, multiplied, etc.\n",
    "- **ToTensor**: Transformation function that converts NumPy array into PyTorch tensor representation. \n",
    "- **training**: The process of adjusting the model's parameters to minimize the loss function.\n",
    "- **validation**: The process of evaluating the model's parameters on a separate dataset to monitor overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c4d64-b868-4310-9471-249f06282ba4",
   "metadata": {},
   "source": [
    "---\n",
    "**Important metrics for evaluating the performance of a model**:\n",
    "- **Accuracy**: The model's overall correctness.\n",
    "  - TP + TN / TP + TN + FP + FN\n",
    "- **Precision**: Accuracy of positive predictions.\n",
    "  - TP / TP + FP\n",
    "- **Recall**: Ability to identify all positive instances.\n",
    "  - TP / TP + FN\n",
    "\n",
    "\n",
    "![Confusion Matrix](https://newbiettn.github.io/images/confusion-matrix-noted.jpg)\n",
    "<br>\n",
    "\n",
    "**Source**: <a href=\"https://newbiettn.github.io/2016/08/30/precision-recall-sensitivity-specificity/\">Ngoc Tran, 2016</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed446e16-823b-4e98-9272-d87835bce8dd",
   "metadata": {},
   "source": [
    "---\n",
    "**Gradient-based Optimization** <br>\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function (error) between predicted and actual outputs.\n",
    "\n",
    "- The graphs below show the loss value (y-axis) as a function of the weights (x-axis)\n",
    "- Bottom of the U is where the loss is minimized (optimal weights).\n",
    "- Gradient refers to the slope of the loss function at a given point on the graph. It measures the rate of change of the loss with respect to the weights.\n",
    "- A very small gradient value close that comes as close to zero as possible is the goal.\n",
    "\n",
    "![Confusion Matrix](https://duchesnay.github.io/pystatsml/_images/learning_rate_choice.png)\n",
    "<br>\n",
    "\n",
    "**Source**: <a href=\"https://duchesnay.github.io/pystatsml/optimization/optim_gradient_descent.html\"> Edouard Duchesnay</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2d7ef-4ce2-45aa-ba9f-cf03119102a7",
   "metadata": {},
   "source": [
    "---\n",
    "**Basics of a Neural Network Model** <br>\n",
    "- **Forward pass**: Get the model predictions by passing input data through the model.\n",
    "- **Loss calculation**: Compute the loss between the predicted and true labels/values.\n",
    "- **Backward pass**: Compute gradient of the loss function with respect to the model parameters.\n",
    "- **Weights update**: Update the weights of the model using an optimizer.\n",
    "\n",
    "\n",
    "![Confusion Matrix](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXAOUqmlyECgfVa81Sr6Ew.png)\n",
    "<br>\n",
    "\n",
    "**Source**: <a href=\"https://medium.com/data-science-365/overview-of-a-neural-networks-learning-process-61690a502fa\"> Rukshan Pramoditha</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3b2a6-f022-4cf1-8ab4-1da1b47b9b6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385e95a-a40d-40cc-8c78-4de0f5785189",
   "metadata": {},
   "source": [
    "#### PyTorch dataset: YelpReviewFull\n",
    "Find more information about this dataset: https://huggingface.co/datasets/Yelp/yelp_review_full\n",
    "\n",
    "**Data Fields**\n",
    "- *text*: The review texts are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
    "- *label*: Corresponds to the score associated with the review (between 1 and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccdf748e-880f-4a0b-a9ae-ff984d8c78d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information\n",
      "________________________________________\n",
      "\n",
      "Type<class 'datasets.dataset_dict.DatasetDict'>\n",
      "\n",
      "Length: 2\n",
      "\n",
      "Dataset structure: {'train': ['label', 'text'], 'test': ['label', 'text']}\n",
      "\n",
      "Dataset overview: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "\n",
      "First row in train set:\n",
      "Label: 4\n",
      "Text: dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "\n",
      "First row in test set:\n",
      "Label: 0\n",
      "Text: I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\\"this time\\\". \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import dataset through Huggingface dataset\n",
    "#ds_yelp = load_dataset(\"Yelp/yelp_review_full\")\n",
    "\n",
    "#Include print statements to see data structure\n",
    "print(\"Dataset information\")\n",
    "print(\"_\" * 40 + \"\\n\")\n",
    "\n",
    "print(f\"Type{(type(ds_yelp))}\")\n",
    "print()\n",
    "\n",
    "print(f\"Length: {len(ds_yelp)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Dataset structure: {ds_yelp.column_names}\")\n",
    "print()\n",
    "\n",
    "print(f\"Dataset overview: {ds_yelp}\")\n",
    "print()\n",
    "\n",
    "print(\"First row in train set:\")\n",
    "print(f\"Label: {ds_yelp['train']['label'][0]}\")\n",
    "print(f\"Text: {ds_yelp['train']['text'][0]}\")\n",
    "print()\n",
    "\n",
    "print(\"First row in test set:\")\n",
    "print(f\"Label: {ds_yelp['test']['label'][0]}\")\n",
    "print(f\"Text: {ds_yelp['test']['text'][0]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bcd60-b6fe-4e14-84c4-843be4f45de8",
   "metadata": {},
   "source": [
    "#### Other relevant pieces of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da92c3-8a7d-4680-8b12-ff59f9be85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "#Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Create an optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #Clear the gradients for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Forward pass: Get model's predictions by passing input data through the model\n",
    "    prediction = model(data)\n",
    "\n",
    "    #Calculate loss: Compute the loss between the predicted labels and the true labels using the loss function\n",
    "    loss = criterion(output, target) #output = model's predicted labels/values | target = tensor of true labels/values\n",
    "\n",
    "    #Backward pass: Compute the gradients of the loss with respect to the model's parameters\n",
    "    loss.backward()\n",
    "\n",
    "    #Updating weights: Use an optimizer to update the model's weights\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf743f-1c74-481d-8c86-e511aed5b142",
   "metadata": {},
   "source": [
    "#### Common structure for deep learning project\n",
    "- **utils.py**: Utility functions for handling hyperparameters, logging, and storing the model.\n",
    "- **model/net.py**: The neural network architecture, the loss function, and evaluation metrics.\n",
    "- **model/data_loader.py**: Data loading, preprocessing, and batching for training and evaluation.\n",
    "- **main.py**: Entry point for the project, includes training (train.py) and evaluation (evaluate.py) of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecb491-65e7-43c7-94e1-a6eac9d2e839",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0.8px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614d506-a500-467c-8a9c-c3d2b576e716",
   "metadata": {},
   "source": [
    "## <span style=\"background-color: #FFE5B4 \"> Section 3. Text Sentiment and Topic Modeling </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fad411-1a81-42b6-bca7-1ab07ec0943b",
   "metadata": {},
   "source": [
    "### General information\n",
    "There are various important packages for *text sentiment and topic modeling*. In the example code below, I will be focusing on:\n",
    "- nltk\n",
    "- spacy\n",
    "- gensim\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb807ef8-9fbb-49c7-96f7-125f518c4bda",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04a43f-8d4f-45c3-9214-a0aeb59a25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages/modules\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "import transformers\n",
    "\n",
    "#Import specific objects\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy import displacy\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af502b1-7261-4ecb-a24a-d7959393c5e3",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">3.1 Text Sentiment</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485289f7-4c55-4e1a-9ee6-2e70e52bc909",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">3.2 Topic Modeling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea99946-e8e3-4f0c-9a9b-9dce7a4cd055",
   "metadata": {},
   "source": [
    "## License and Copyright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a8844-808d-4207-8fcf-122d74389b8a",
   "metadata": {},
   "source": [
    "© 2024 Noor de Bruijn. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-myenv",
   "language": "python",
   "name": "data-analysis-myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
