{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee3a91e-a61e-4bee-892e-39c48504721c",
   "metadata": {},
   "source": [
    "# Data Analysis Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd32e9-b78b-4a71-9663-b958235f5b66",
   "metadata": {},
   "source": [
    "This notebook is part of my data analysis portfolio, where I explore **three** key areas:\n",
    "1. Data Processing and Visualization\n",
    "2. Traditional Machine Learning and Deep Learning\n",
    "3. Text Sentiment and Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b13ebcc-e748-4be4-bcad-913d39335fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import general packages\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc6c66-b979-4a0f-9746-deb18aaacab1",
   "metadata": {},
   "source": [
    "## <span style=\"background-color: #FFE5B4 \"> Section 2. Traditional Machine Learning and Deep Learning </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34669a01-d0ee-44b6-a16d-958a97da61fe",
   "metadata": {},
   "source": [
    "### General information\n",
    "There are various important packages for *traditional machine learning and deep learning*. In the example code below, I will be focusing on:\n",
    "- pytorch\n",
    "- scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2139b-0343-41ee-884d-615ec4461010",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c29ffa8-1e6c-4c90-b0fd-2527aeada754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import packages/modules\n",
    "import sklearn as sk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Import specific objects\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de3563-ca31-4af8-aad0-ef66763b6648",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">2.1 Traditional Machine Learning</span>\n",
    "scikit-learn is focused on traditional machine learning tasks, such as linear regression, clustering, and support vector machines (CVMs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1efb62-8246-44f5-a825-67de2892d6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba05eaf-1b24-438d-bb8f-3da5c2545ec2",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #FFE5B4 \">2.2 Deep Learning</span>\n",
    "PyTorch is primarily designed for deep learning tasks, such as neural networks (CNNs, RNNs) and transformers (BERT, RoBERTa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ce277-5b3b-44bb-b743-f04d5c8ec9e5",
   "metadata": {},
   "source": [
    "#### Important terminology: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab285f58-46ef-4424-b3ad-6b814e007ced",
   "metadata": {},
   "source": [
    "- **autograd**: Computes the gradients (slopes) of the loss function with respect to the model's weights.\n",
    "  - Important: When a forward function is defined in PyTorch, the backward function is automatically generated by PyTorch's autograd system, so the backward function doesn't need to be explicitly defined.\n",
    "- **backpropagation**: The process of adjusting the weights of a neural network by analyzing the error rate from the previous iteration.\n",
    "- **batch**: A hyperparameter that defines the number of samples that are processed before the interal model parameters are updated.\n",
    "- **Dataset**: Data primitive that stores the samples and their corresponding labels.\n",
    "- **DataLoader**: Data primitive that wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "- **gradient descent**: An iterative optimization method that minimises the loss function in machine learning models.\n",
    "- **epoch**: A hyperparameter that defines the number of complete passes through the training dataset.\n",
    "- **hyperparameter**: A parameter that is set before the machine learning process begins.\n",
    "- **learning rate**: A hyperparameter that controls the step size of each gradient descent update.\n",
    "- **loss function**: A mathematical function that measures the difference between the model's predictions and the actual labels. In other words, it computes a value that estimates how far the output is from the target.\n",
    "  - Common loss functions: nn.CrossEntropyLoss() | nn.MSELoss() | nn.BCELoss() | nn.L1Loss()\n",
    "- **Model**: A neural network architecture that is designed to solve a specific problem.\n",
    "- **module**: Base class for all neural network models (the building blocks).\n",
    "- **neural network (NN)**: A machine learning program/model that makes decisions in a manner similar to the human brain.\n",
    "  - **Feedforward Networks**: Data flows in only one direction, from input layer to output layer, with no feedback loops.\n",
    "  - **Recurrent Neural Networks (RNNs)**: Data flows in a loop, processing sequential data (text, audio, time series).\n",
    "  - **Convolutional Neural Networks (CNNs)**: Data flows through convolutional and pooling layers to extract features from grid-like data (images).\n",
    "  - **Transformers**: Data flows through self-attention mechanisms and encoder-decoder structures to process sequential data. Where RNNs process sequential data one element at a time, transformers processes entire sequences simultaneously (parallel processing).\n",
    "- **optimizer**: A tool that helps with the process of training a machine learning model.\n",
    "  - SGD (stochastic gradient descent) is an optimizer that updates model weights based on the gradient of the loss function.\n",
    "  - torch.optim provides a wide range of optimizers.\n",
    "- **parameter**: \n",
    "- **propagation**\n",
    "    - **forward propagation**: NN makes best guess about the correct output. It runs the input through each of the functions to make this guess.\n",
    "    - **backward propagation**: NN adjusts its parameters proportionate to the error in its guess, making bigger/smaller changes for bigger/smaller errors. Backpropagation relies on autograd to compute gradients, which are then used to update the model's weights.\n",
    "    - loss.backward() implicitly generates the backward function.\n",
    "- **sample**: A single row of data.\n",
    "- **tensor**: A multi-dimensional array of numerical values (a \"container\" for data) that run on GPU to accelerate computing.\n",
    "    - A tensor can be created by running: torch.tensor(data) | torch.ones(r,c) | torch.zero(r,c) | torch.rand(r,c).\n",
    "    - A tensor can also be created from a numpy array by running: torch.from_numpy(np_array)\n",
    "    - Tensors of similar shapes can be added, multiplied, etc.\n",
    "- **ToTensor**: Transformation function that converts NumPy array into PyTorch tensor representation. \n",
    "- **training**: The process of adjusting the model's parameters to minimize the loss function.\n",
    "- **validation**: The process of evaluating the model's parameters on a separate dataset to monitor overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c4d64-b868-4310-9471-249f06282ba4",
   "metadata": {},
   "source": [
    "---\n",
    "**Important metrics for evaluating the performance of a model**:\n",
    "- **Accuracy**: The model's overall correctness.\n",
    "  - TP + TN / TP + TN + FP + FN\n",
    "- **Precision**: Accuracy of positive predictions.\n",
    "  - TP / TP + FP\n",
    "- **Recall**: Ability to identify all positive instances.\n",
    "  - TP / TP + FN\n",
    "\n",
    "\n",
    "![Confusion Matrix](https://newbiettn.github.io/images/confusion-matrix-noted.jpg)\n",
    "<br>\n",
    "\n",
    "**Source**: <a href=\"https://newbiettn.github.io/2016/08/30/precision-recall-sensitivity-specificity/\">Ngoc Tran, 2016</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed446e16-823b-4e98-9272-d87835bce8dd",
   "metadata": {},
   "source": [
    "---\n",
    "**Gradient-based Optimization** <br>\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function (error) between predicted and actual outputs.\n",
    "\n",
    "- The graphs below show the loss value (y-axis) as a function of the weights (x-axis)\n",
    "- Bottom of the U is where the loss is minimized (optimal weights).\n",
    "- Gradient refers to the slope of the loss function at a given point on the graph. It measures the rate of change of the loss with respect to the weights.\n",
    "- A very small gradient value close that comes as close to zero as possible is the goal.\n",
    "\n",
    "![Confusion Matrix](https://duchesnay.github.io/pystatsml/_images/learning_rate_choice.png)\n",
    "<br>\n",
    "\n",
    "**Source**: <a href=\"https://duchesnay.github.io/pystatsml/optimization/optim_gradient_descent.html\"> Edouard Duchesnay</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2d7ef-4ce2-45aa-ba9f-cf03119102a7",
   "metadata": {},
   "source": [
    "---\n",
    "**Basics of a Neural Network Model** <br>\n",
    "- **Forward pass**: Get the model predictions by passing input data through the model.\n",
    "- **Loss calculation**: Compute the loss between the predicted and true labels/values.\n",
    "- **Backward pass**: Compute gradient of the loss function with respect to the model parameters.\n",
    "- **Weights update**: Update the weights of the model using an optimizer.\n",
    "\n",
    "\n",
    "![Confusion Matrix](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXAOUqmlyECgfVa81Sr6Ew.png)\n",
    "<br>\n",
    "\n",
    "**Source**: <a href=\"https://medium.com/data-science-365/overview-of-a-neural-networks-learning-process-61690a502fa\"> Rukshan Pramoditha, 2022</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3b2a6-f022-4cf1-8ab4-1da1b47b9b6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385e95a-a40d-40cc-8c78-4de0f5785189",
   "metadata": {},
   "source": [
    "#### PyTorch dataset: YelpReviewFull\n",
    "Find more information about this dataset: https://huggingface.co/datasets/Yelp/yelp_review_full\n",
    "\n",
    "**Data Fields**\n",
    "- *text*: The review texts are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
    "- *label*: Corresponds to the score associated with the review (between 1 and 5).\n",
    "\n",
    "For personal reference: A similar dataset is ag_news (text and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048aaa13-db9e-4a45-bff8-6f400642e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#ds = load_dataset(\"fancyzhx/ag_news\")\n",
    "\n",
    "print(ds)\n",
    "\n",
    "print(iter(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdf748e-880f-4a0b-a9ae-ff984d8c78d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information\n",
      "________________________________________\n",
      "\n",
      "Type<class 'datasets.dataset_dict.DatasetDict'>\n",
      "\n",
      "Length: 2\n",
      "\n",
      "Dataset structure: {'train': ['label', 'text'], 'test': ['label', 'text']}\n",
      "\n",
      "Dataset overview: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "\n",
      "Structure of first dataset: Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 650000\n",
      "})\n",
      "Structure of second dataset: Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Import dataset through Huggingface dataset\n",
    "ds_yelp = load_dataset(\"Yelp/yelp_review_full\")\n",
    "\n",
    "#Include print statements to see data structure\n",
    "print(\"Dataset information\")\n",
    "print(\"_\" * 40 + \"\\n\")\n",
    "\n",
    "print(f\"Type{(type(ds_yelp))}\")\n",
    "print()\n",
    "\n",
    "print(f\"Length: {len(ds_yelp)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Dataset structure: {ds_yelp.column_names}\")\n",
    "print()\n",
    "\n",
    "print(f\"Dataset overview: {ds_yelp}\")\n",
    "print()\n",
    "\n",
    "print(f\"Structure of first dataset: {ds_yelp['train']}\")\n",
    "print(f\"Structure of second dataset: {ds_yelp['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fa148f-63fc-4cc1-af1c-4c418593183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 4, 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"}\n",
      "\n",
      "{'label': 1, 'text': \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\"}\n",
      "\n",
      "{'label': 3, 'text': \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\"}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'label': 0, 'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'}\n",
      "\n",
      "{'label': 0, 'text': \"Don't waste your time.  We had two different people come to our house to give us estimates for a deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\"}\n",
      "\n",
      "{'label': 0, 'text': 'All I can say is the worst! We were the only 2 people in the place for lunch, the place was freezing and loaded with kids toys! 2 bicycles, a scooter, and an electronic keyboard graced the dining room. A fish tank with filthy, slimy fingerprints smeared all over it is there for your enjoyment.\\\\n\\\\nOur food came... no water to drink, no tea, medium temperature food. Of course its cold, just like the room, I never took my jacket off! The plates are too small, you food spills over onto some semi-clean tables as you sit in your completely worn out booth seat. The fried noodles were out of a box and nasty, the shrimp was mushy, the fried rice was bright yellow.\\\\n\\\\nWe asked for water, they brought us 1 in a SOLO cup for 2 people. I asked for hot tea, they said 10 minutes. What Chinese restaurant does not have hot tea available upon request?\\\\n\\\\nOver all.... my first and last visit to this place. The only good point was that it was cheap, and deservingly so.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create an train_iter to iterate through the training items\n",
    "train_iter = iter(ds_yelp[\"train\"])\n",
    "test_iter = iter(ds_yelp[\"test\"])\n",
    "\n",
    "for i in range(3):\n",
    "    print(next(train_iter))\n",
    "    print()\n",
    "\n",
    "print(\"--\" * 68)\n",
    "\n",
    "for i in range(3):\n",
    "    print(next(test_iter))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce41afb-aed8-4c31-9d3c-3d2cdcafbd4d",
   "metadata": {},
   "source": [
    "#### Step-by-step guide to define the neural network model\n",
    "\n",
    "#### Step 1. Text preprocessing\n",
    "- Text cleaning\n",
    "- Tokenization\n",
    "- Build vocabulary\n",
    "- Create pipelines\n",
    "- Split the data into training and validation sets\n",
    "\n",
    "#### Step 2. Define the neural network\n",
    "- Pick suitable architecture\n",
    "- Define input layer (text embeddings), hidden layer, output layer (star prediction)\n",
    "\n",
    "#### Step 3. Compile the model\n",
    "- Choose a loss function \n",
    "- Select an optimizer\n",
    "- Define the evaluation metrics\n",
    "\n",
    "#### Step 4. Train the model\n",
    "- Divide the training data into batches using DataLoader\n",
    "- Use the training data to train the model\n",
    "- Forward propagation: Pass the input data through the network to get predictions\n",
    "- Calculate the loss between predictions and actual labels\n",
    "- Backward propagation: Update the weights using the optimizer and loss\n",
    "\n",
    "#### Step 5. Evaluate the model\n",
    "- Use the validation data to evaluate the model's performance\n",
    "- Calculate metrics: Accuracy, F1-score, and loss\n",
    "\n",
    "#### Step 6. Fine-tune the model\n",
    "- Adjust hyperparameters to improve performance\n",
    "- Experiment with different architectures or techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf46f4-20ef-411c-8396-3a05cd6b5f88",
   "metadata": {},
   "source": [
    "#### Step 1. Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba87ba6-36b1-47d7-8397-9bd6f0e52055",
   "metadata": {},
   "source": [
    "##### **Text cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90716a3b-b3b6-4a70-9bf6-2a52fba664cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Remove non-words and non-space characters\n",
    "for i, text in enumerate(ds_yelp['train']['text']):\n",
    "    ds_yelp['train']['text'][i] = re.sub(r\"[^\\w\\s.']\", '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701c4c0-7977-409b-9be1-27ee6fd9d472",
   "metadata": {},
   "source": [
    "##### **Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1b447-5a17-45bc-a345-85d6591d0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Define a set of english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i, text in enumerate(ds_yelp['train']['text']):\n",
    "    words = text.split()\n",
    "    #Remember: List comprehension [expression for variable in iterable if condition]\n",
    "    #Expression is a placeholder/alias for the value that is being produced and added to the words list\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    #Storing the cleaned text back in the dataset\n",
    "    ds_yelp['train']['text'][i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9730e-3fe3-476d-b7d6-afb6bf688733",
   "metadata": {},
   "source": [
    "##### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30931911-63fe-4e66-bddc-1bba1d63e442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "#Define a function for tokenization\n",
    "def tokenize_text(text):\n",
    "    #Load pre-built tokenizer\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    return tokenizer(text)\n",
    "\n",
    "#Apply tokenization to the training data\n",
    "for i, text in enumerate(ds_yelp['train']['text']):\n",
    "    tokens = tokenize_text(text)\n",
    "    ds_yelp['train']['text'][i] = tokens\n",
    "    \n",
    "#Print the structure of tokenized_data\n",
    "print(tokenized_data, end ='\\n\\n')\n",
    "\n",
    "# Print the structure of each dataset\n",
    "print(f\"Example of tokens in train dataset: {ds_yelp['train']['text'][0]}, end='\\n\\n'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310dfe0-b82a-4688-bd82-b21ab3a37ad8",
   "metadata": {},
   "source": [
    "##### **Padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1225aa3-ff1c-4602-9727-6180393b8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "#Define a function for padding\n",
    "def pad_tokens(tokens):\n",
    "    #By default, the pad_sequence function pads the sequences to the length of the longest sequence in the batch\n",
    "    return pad_sequence([tokens], batch_first=True)\n",
    "\n",
    "#Apply padding to the tokenized training data\n",
    "for i, tokens in enumerate(ds_yelp['train']['text']):\n",
    "    padded_tokens = pad_tokens(tokens)\n",
    "    ds_yelp['train']['text'][i] = padded_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae9fb7-bab7-4bfa-95e4-a7e035aafb2c",
   "metadata": {},
   "source": [
    "##### **Build vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a436b-5fab-4982-adad-776f33fc4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the training dataset\n",
    "train_dataset = [tokenized_data['train'][0]['tokens']]\n",
    "#train_dataset = [tokenized_data['train']['tokens']]\n",
    "\n",
    "#Build a vocabulary with the raw training dataset\n",
    "vocab = build_vocab_from_iterator(train_dataset, specials=[\"<unk>\"])\n",
    "\n",
    "#Setting the default index to <unk>, so when we encounter an unknown world in new data it's replaced with the <unk> token\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "#Replace placeholder PAD_IDX with actual index of the <PAD> token\n",
    "PAD_IDX = vocab['<PAD>']\n",
    "\n",
    "#Print dictionary where the keys are the tokens and the values their indices\n",
    "print(vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a0c68-db8b-4643-a617-605062a89e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8986fa3-51b2-4a05-b6a4-b575594c4fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88d248-516f-4a28-b4bc-b4cae8c9d742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdb2ef3c-5c5b-4ab0-acda-fc4564fae158",
   "metadata": {},
   "source": [
    "##### **Create pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3c4e6-b590-44f5-96dd-de9e8b6466d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-words and non-space characters\n",
    "cleaning_pipeline = lambda x: re.sub(r\"[^\\w\\s.]\", '', x)\n",
    "\n",
    "#Tokenize the incoming text data and look each token up in the vocab dictionary\n",
    "tokenizer_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "#Convert label data to numerical indices (by extracting 1 we are ensuring the labels align with zero-based idnexing)\n",
    "label_pipeline = lambda x: int(x) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea8e1c-4881-4f82-a4ee-c29e94828b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b50a1fc7-093b-4fe1-aedf-251018c35e28",
   "metadata": {},
   "source": [
    "#### Step 2. Define the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c2d0d-99f5-461b-adbd-20b5f8efae93",
   "metadata": {},
   "source": [
    "#### Step 3. Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06388f0f-8eba-4214-9179-1984e8e5a7df",
   "metadata": {},
   "source": [
    "#### Step 4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db230ef5-8419-49c4-ac11-92e730cda9cb",
   "metadata": {},
   "source": [
    "##### **Divide the training data into batches using DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7dcd3-a182-4631-8020-c163bb8e118e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "067100ba-2b55-428c-a9ac-d784935fbdb3",
   "metadata": {},
   "source": [
    "#### Step 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c1569-d8ab-4950-84e1-c6a52ff35363",
   "metadata": {},
   "source": [
    "#### Step 6. Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bcd60-b6fe-4e14-84c4-843be4f45de8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Other relevant pieces of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da92c3-8a7d-4680-8b12-ff59f9be85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "#Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Create an optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #Clear the gradients for the next iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Forward pass: Get model's predictions by passing input data through the model\n",
    "    prediction = model(data)\n",
    "\n",
    "    #Calculate loss: Compute the loss between the predicted labels and the true labels using the loss function\n",
    "    loss = criterion(output, target) #output = model's predicted labels/values | target = tensor of true labels/values\n",
    "\n",
    "    #Backward pass: Compute the gradients of the loss with respect to the model's parameters\n",
    "    loss.backward()\n",
    "\n",
    "    #Updating weights: Use an optimizer to update the model's weights\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf743f-1c74-481d-8c86-e511aed5b142",
   "metadata": {},
   "source": [
    "#### Common structure for deep learning project\n",
    "- **utils.py**: Utility functions for handling hyperparameters, logging, and storing the model.\n",
    "- **model/net.py**: The neural network architecture, the loss function, and evaluation metrics.\n",
    "- **model/data_loader.py**: Data loading, preprocessing, and batching for training and evaluation.\n",
    "- **main.py**: Entry point for the project, includes training (train.py) and evaluation (evaluate.py) of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecb491-65e7-43c7-94e1-a6eac9d2e839",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0.8px solid black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea99946-e8e3-4f0c-9a9b-9dce7a4cd055",
   "metadata": {},
   "source": [
    "## License and Copyright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a8844-808d-4207-8fcf-122d74389b8a",
   "metadata": {},
   "source": [
    "Â© 2024 Noor de Bruijn. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-myenv",
   "language": "python",
   "name": "data-analysis-myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
